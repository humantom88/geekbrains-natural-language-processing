{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW-3-colab.ipynb","provenance":[{"file_id":"1o65wrq6RYgWyyMvNP8r9ZknXBniDoXrn","timestamp":1617176736329}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wWzU9hqUomdU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617177000113,"user_tz":-180,"elapsed":26151,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"c305e141-95ae-4580-a705-56e6fb186001"},"source":["!pip install -q --upgrade nltk gensim bokeh pandas\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.4MB 11.5MB/s \n","\u001b[K     |████████████████████████████████| 23.9MB 1.6MB/s \n","\u001b[K     |████████████████████████████████| 9.9MB 50.7MB/s \n","\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.3 which is incompatible.\u001b[0m\n"],"name":"stdout"},{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"K-X7I7nc1gyS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617177008639,"user_tz":-180,"elapsed":1044,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"f05d7662-4182-4b8d-a975-71ea8edae753"},"source":["import pandas as pd\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from gensim.models import Word2Vec\n","from sklearn.manifold import TSNE"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n","  warnings.warn(msg)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"UoYjVExHd_OC","executionInfo":{"status":"ok","timestamp":1617177027405,"user_tz":-180,"elapsed":850,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}}},"source":["import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.corpus import stopwords\n","from string import punctuation"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xvqrFUS6vVhh"},"source":["## Запилим пословный машинный перевод!"]},{"cell_type":"code","metadata":{"id":"1CXcr-ypzGXg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617177101176,"user_tz":-180,"elapsed":69794,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"2d8bab73-9bba-42d3-cdd4-98773c9ed6c6"},"source":["!wget -O ukr_rus.train.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vAK0SWXUqei4zTimMvIhH3ufGPsbnC_O\"\n","!wget -O ukr_rus.test.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1W9R2F8OeKHXruo2sicZ6FgBJUTJc8Us_\"\n","!wget -O fairy_tale.txt -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1sq8zSroFeg_afw-60OmY8RATdu_T1tej\"\n","\n","# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once per notebook.\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","downloaded = drive.CreateFile({'id': '1d7OXuil646jUeDS1JNhP9XWlZogv6rbu'})\n","downloaded.GetContentFile('cc.ru.300.vec.zip')\n","\n","downloaded = drive.CreateFile({'id': '1yAqwqgUHtMSfGS99WLGe5unSCyIXfIxi'})\n","downloaded.GetContentFile('cc.uk.300.vec.zip')\n","\n","!unzip cc.ru.300.vec.zip\n","!unzip cc.uk.300.vec.zip"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Archive:  cc.ru.300.vec.zip\n","  inflating: cc.ru.300.vec           \n","Archive:  cc.uk.300.vec.zip\n","  inflating: cc.uk.300.vec           \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7RqUeOXxws8y"},"source":["Напишем простенькую реализацию модели машинного перевода.\n","\n","Идея основана на статье [Word Translation Without Parallel Data](https://arxiv.org/pdf/1710.04087.pdf). У авторов в репозитории еще много интересного: [https://github.com/facebookresearch/MUSE](https://github.com/facebookresearch/MUSE).\n","\n","А мы будем переводить с украинского на русский.\n","\n","![](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/blue_cat_blue_whale.png)   \n","*синій кіт* vs. *синій кит*"]},{"cell_type":"code","metadata":{"id":"jjPj9FTRry0U","executionInfo":{"status":"ok","timestamp":1617177652467,"user_tz":-180,"elapsed":533797,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}}},"source":["from gensim.models import KeyedVectors\n","\n","ru_emb = KeyedVectors.load_word2vec_format(\"cc.ru.300.vec\")\n","uk_emb = KeyedVectors.load_word2vec_format(\"cc.uk.300.vec\")"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rGx4TXWFJ65"},"source":["Посмотрим на пару серпень-август (являющихся переводом)"]},{"cell_type":"code","metadata":{"id":"FkHer36xyh4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617177676608,"user_tz":-180,"elapsed":1386,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"e0aa13d6-02ee-4d3d-dbe1-6aad9f3cb802"},"source":["ru_emb.most_similar([ru_emb[\"август\"]])"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('август', 1.0000001192092896),\n"," ('июль', 0.9383152723312378),\n"," ('сентябрь', 0.9240029454231262),\n"," ('июнь', 0.9222574830055237),\n"," ('октябрь', 0.9095539450645447),\n"," ('ноябрь', 0.8930036425590515),\n"," ('апрель', 0.8729087114334106),\n"," ('декабрь', 0.8652557730674744),\n"," ('март', 0.8545795679092407),\n"," ('февраль', 0.8401415944099426)]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"1RSDixWvylEP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617177689947,"user_tz":-180,"elapsed":824,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"dc6fda23-f184-49ed-95cf-23f469a3f7c8"},"source":["uk_emb.most_similar([uk_emb[\"серпень\"]])"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('серпень', 0.9999998807907104),\n"," ('липень', 0.9096441268920898),\n"," ('вересень', 0.9016969203948975),\n"," ('червень', 0.8992518782615662),\n"," ('жовтень', 0.8810408115386963),\n"," ('листопад', 0.8787633180618286),\n"," ('квітень', 0.8592804670333862),\n"," ('грудень', 0.8586863279342651),\n"," ('травень', 0.840811014175415),\n"," ('лютий', 0.8256431221961975)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"iwmm3YQ1yl1U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617177697715,"user_tz":-180,"elapsed":981,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}},"outputId":"8369591e-4e7f-4686-c208-a9bbafbfa1fc"},"source":["ru_emb.most_similar([uk_emb[\"серпень\"]])"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Недопустимость', 0.24435284733772278),\n"," ('конструктивность', 0.23293082416057587),\n"," ('офор', 0.23256804049015045),\n"," ('deteydlya', 0.230317160487175),\n"," ('пресечении', 0.22632381319999695),\n"," ('одностороннего', 0.22608886659145355),\n"," ('подход', 0.2230587750673294),\n"," ('иболее', 0.22003726661205292),\n"," ('2015Александр', 0.21872766315937042),\n"," ('конструктивен', 0.21796567738056183)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"lAsW7oxszE_I","executionInfo":{"status":"ok","timestamp":1617177712013,"user_tz":-180,"elapsed":5510,"user":{"displayName":"Roman Zakharov","photoUrl":"","userId":"18255168926005506833"}}},"source":["def load_word_pairs(filename):\n","    uk_ru_pairs = []\n","    uk_vectors = []\n","    ru_vectors = []\n","    with open(filename, \"r\", encoding='utf8') as inpf:\n","        for line in inpf:\n","            uk, ru = line.rstrip().split(\"\\t\")\n","            if uk not in uk_emb or ru not in ru_emb:\n","                continue\n","            uk_ru_pairs.append((uk, ru))\n","            uk_vectors.append(uk_emb[uk])\n","            ru_vectors.append(ru_emb[ru])\n","    return uk_ru_pairs, np.array(uk_vectors), np.array(ru_vectors)\n","\n","\n","uk_ru_train, X_train, Y_train = load_word_pairs(\"ukr_rus.train.txt\")\n","uk_ru_test, X_test, Y_test = load_word_pairs(\"ukr_rus.test.txt\")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9z6ts7DC0XmN"},"source":["### Учим маппинг из одного пространства эмбеддингов в другое\n","\n","У нас есть пары слов, соответствующих друг другу, и их эмбеддинги. Найдем преобразование из одного пространства в другое, чтобы приблизить известные нам слова:\n","\n","$$W^*= \\arg\\min_W ||WX - Y||_F, \\text{где} ||*||_F - \\text{норма Фробениуса}$$\n","\n","Эта функция очень похожа на линейную регрессию (без биаса).\n","\n","**Задание** Реализуйте её - воспользуйтесь `LinearRegression` из sklearn с `fit_intercept=False`:"]},{"cell_type":"code","metadata":{"id":"fraTOQtu1YWI"},"source":["mapping = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PrzRk3ja1b_6"},"source":["Проверим, куда перейдет `серпень`:"]},{"cell_type":"code","metadata":{"id":"Quax6HnF1aON"},"source":["august = mapping.predict(uk_emb[\"серпень\"].reshape(1, -1))\n","ru_emb.most_similar(august)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ih1GLNZt1nZX"},"source":["Должно получиться, что в топе содержатся разные месяцы, но август не первый.\n","\n","Будем мерять percision top-k с k = 1, 5, 10.\n","\n","**Задание** Реализуйте следующую функцию:"]},{"cell_type":"code","metadata":{"id":"JnmrLp9y2gNI"},"source":["def precision(pairs, mapped_vectors, topn=1):\n","    \"\"\"\n","    :args:\n","        pairs = list of right word pairs [(uk_word_0, ru_word_0), ...]\n","        mapped_vectors = list of embeddings after mapping from source embedding space to destination embedding space\n","        topn = the number of nearest neighbours in destination embedding space to choose from\n","    :returns:\n","        precision_val, float number, total number of words for those we can find right translation at top K.\n","    \"\"\"\n","    assert len(pairs) == len(ru_vectors)\n","    num_matches = 0\n","    for i, (_, ru) in enumerate(pairs):\n","        <write code here>\n","    precision_val = num_matches / len(pairs)\n","    return precision_val"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1NIvhSH2olG"},"source":["assert precision([(\"серпень\", \"август\")], august, topn=5) == 0.0\n","assert precision([(\"серпень\", \"август\")], august, topn=9) == 1.0\n","assert precision([(\"серпень\", \"август\")], august, topn=10) == 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ml_w1Tl2r7Y"},"source":["assert precision(uk_ru_test, X_test) == 0.0\n","assert precision(uk_ru_test, Y_test) == 1.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-d9KQHMr2tx8"},"source":["precision_top1 = precision(uk_ru_test, mapping.predict(X_test), 1)\n","precision_top5 = precision(uk_ru_test, mapping.predict(X_test), 5)\n","\n","assert precision_top1 >= 0.635\n","assert precision_top5 >= 0.813"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JNbDTP502urT"},"source":["### Улучшаем маппинг\n","\n","Можно показать, что маппинг лучше строить ортогональным:\n","$$W^*= \\arg\\min_W ||WX - Y||_F \\text{, где: } W^TW = I$$\n","\n","Искать его можно через SVD:\n","$$X^TY=U\\Sigma V^T\\text{, singular value decompostion}$$\n","\n","$$W^*=UV^T$$\n","\n","**Задание** Реализуйте эту функцию."]},{"cell_type":"code","metadata":{"id":"9de8XZ_F3v53"},"source":["def learn_transform(X_train, Y_train):\n","    \"\"\" \n","    :returns: W* : float matrix[emb_dim x emb_dim] as defined in formulae above\n","    \"\"\"\n","    <write code there>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WeCadzN382y"},"source":["W = learn_transform(X_train, Y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6qaMb0E3-f9"},"source":["ru_emb.most_similar([np.matmul(uk_emb[\"серпень\"], W)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Nn58crh4AH0"},"source":["assert precision(uk_ru_test, np.matmul(X_test, W)) >= 0.653\n","assert precision(uk_ru_test, np.matmul(X_test, W), 5) >= 0.824"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lqgcYk-c4DE5"},"source":["### Пишем переводчик"]},{"cell_type":"markdown","metadata":{"id":"hwi70fP6FaAN"},"source":["Реализуем простой пословный переводчик - для каждого слова будем искать его ближайшего соседа в общем пространстве эмбеддингов. Если слова нет в эмбеддингах - просто копируем его."]},{"cell_type":"code","metadata":{"id":"0etAHUks4JOr"},"source":["with open(\"fairy_tale.txt\", \"r\") as in f:\n","    uk_sentences = [line.rstrip().lower() for line in in f]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JK_FJGmn4N7V"},"source":["def translate(sentence):\n","    \"\"\"\n","    :args:\n","        sentence - sentence in Ukrainian (str)\n","    :returns:\n","        translation - sentence in Russian (str)\n","\n","    * find ukrainian embedding for each word in sentence\n","    * transform ukrainian embedding vector\n","    * find nearest russian word and replace\n","    \"\"\"\n","    <implement it!>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H47pbFyk4P6D"},"source":["assert translate(\".\") == \".\"\n","assert translate(\"1 , 3\") == \"1 , 3\"\n","assert translate(\"кіт зловив мишу\") == \"кот поймал мышку\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PAVWK7mE4RYU"},"source":["for sentence in uk_sentences:\n","    print(\"src: {}\\ndst: {}\\n\".format(sentence, translate(sentence)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5GrChTeFqIg"},"source":["# Дополнительные материалы"]},{"cell_type":"markdown","metadata":{"id":"HwffxpbmFwDh"},"source":["## Почитать\n","### База:  \n","[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n","[Deep Learning, NLP, and Representations, Christopher Olah](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)  \n","\n","### Как кластеризовать смыслы многозначных слов:  \n","[Making Sense of Word Embeddings (2016), Pelevina et al](http://anthology.aclweb.org/W16-1620)    \n","\n","### Как оценивать эмбеддинги\n","[Evaluation methods for unsupervised word embeddings (2015), T. Schnabel](http://www.aclweb.org/anthology/D15-1036)  \n","[Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance (2016), B. Chiu](https://www.aclweb.org/anthology/W/W16/W16-2501.pdf)  \n","[Problems With Evaluation of Word Embeddings Using Word Similarity Tasks (2016), M. Faruqui](https://arxiv.org/pdf/1605.02276.pdf)  \n","[Improving Reliability of Word Similarity Evaluation by Redesigning Annotation Task and Performance Measure (2016), Oded Avraham, Yoav Goldberg](https://arxiv.org/pdf/1611.03641.pdf)  \n","[Evaluating Word Embeddings Using a Representative Suite of Practical Tasks (2016), N. Nayak](https://cs.stanford.edu/~angeli/papers/2016-acl-veceval.pdf)  \n","\n","\n","## Посмотреть\n","[Word Vector Representations: word2vec, Lecture 2, cs224n](https://www.youtube.com/watch?v=ERibwqs9p38)"]}]}