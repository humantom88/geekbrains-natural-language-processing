{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Рекурентные сети для обработки последовательностей\n",
    "\n",
    "Вспомним все, что мы уже знаем про обработку текстов:\n",
    "- Компьютер не понимает текст, поэтому нам нужно его как-то закодировать - представить в виде вектора\n",
    "- В тексте много повторяющихся слов/лишний слов - нужно сделать препроцессинг:\n",
    "    - удалить знаки препинания\n",
    "    - удалить стоп-слова\n",
    "    - привести слова к начальной форме (**стемминг** и **лемматизация**)\n",
    "    - ???\n",
    "    \n",
    "    \n",
    "- После этого мы можем представить наш текст (набор слов) в виде вектора, например, стандартными способами:\n",
    "    - **CounterEncoding** - вектор длины размер нашего словаря\n",
    "        - есть словарь vocab, который можем включать слова, ngram-ы\n",
    "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=1,\\ если\\ vocab[i]\\ \\in\\ doc$\n",
    "    - **HashingVectorizer** - вектор заранее заданной длины\n",
    "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=1,\\ если\\ \\exists\\ txt\\ \\in\\ doc:\\ hash(text)\\ =\\ i$\n",
    "    - **TfIdfVectorizer** - вектор длины размер нашего словаря\n",
    "        - есть словарь vocab, который можем включать слова, ngram-ы\n",
    "        - каждому документу $doc$ ставим в соответствие вектор $vec\\ :\\ vec[i]=tf(vocab[i])*idf(vocab[i]),\\ если\\ vocab[i]\\ \\in\\ doc$\n",
    "    \n",
    "        $$ tf(t,\\ d)\\ =\\ \\frac{n_t}{\\sum_kn_k} $$\n",
    "        $$ idf(t,\\ D)\\ =\\ \\log\\frac{|D|}{|\\{d_i\\ \\in\\ D|t\\ \\in\\ D\\}|} $$\n",
    "        \n",
    ", где \n",
    "- $n_t$ - число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе\n",
    "- $|D|$ — число документов в коллекции;\n",
    "- $|\\{d_i\\ \\in\\ D\\mid\\ t\\in d_i\\}|$— число документов из коллекции $D$, в которых встречается $t$ (когда $n_t\\ \\neq\\ 0$).\n",
    "\n",
    "\n",
    "\n",
    "Это база и она работает. Мы изучили более продвинутые подходы: эмбединги и сверточные сети по эмбедингам. Но тут есть проблема: любой текст - это последовательность, ни эмбединги, ни сверточные сети не работают с ним как с последовательностью. Так давайте попробуем придумать архитектуру, которая будет работать с текстом как с последовательностью, двигаясь по эмбедингам и как-то меняя их значения."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Придумаем сами архитектуру, чтобы работать с последовательностями\n",
    "\n",
    "Возьмем перцептрон с входом - эмбедингом слова (пусть пока он фиксированный) и будем пытаться классифицировать каждое слово.\n",
    "\n",
    "Почему классифицировать? потому что это частая задача в обработке языка + это дает возможность генерировать текст (просто классифицируем на кол-во классов = кол-ву слов в словаре).\n",
    "\n",
    "<img src=\"images/Single_layer_perceptron.png\">\n",
    "\n",
    "Какая тут последовательность? никакой, но давайте на вход подавать эмбединг, но в 1 скрытый слой будем добавлять последний скрытый слой предыдущего шага)\n",
    "\n",
    "<img src=\"images/Rnnbr.png\">\n",
    "\n",
    "То есть мы прокидываем информацию с предыдущего шага, а за счет того, что мы все время так стекаем вектора мы получаем то, что информация проходит через текст от начала до конца. Что делать с 1 шагом? -> Добавим вектор из нулей. И вот мы получили первую рекурентную сеть. Чаще её рисуют следующим образом:\n",
    "\n",
    "\n",
    "<img src=\"images/Rnn.png\">\n",
    "\n",
    "Итак, мы придумали простую рекуретную сеть. Последний открытый вопрос как её обучать?\n",
    "\n",
    "Все также, градиентным спуском, нам нужно двигаться во времени и обновлять параметры, поэтому обучение таких сетей занимает очень много времени (вы не можете обновить веса для 1-го токена, пока не посчитаете градиент сквозь время).\n",
    "\n",
    "\n",
    "Что делать, если мы хотим классифицировать текст целиком? оставить только последний выход!\n",
    "\n",
    "<img src=\"images/RnnTasks.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# попробуем запрограммировать простую рекурентную сеть. Возьмем датасет с прошлого занятия\n",
    "\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "\n",
    "df_train = pd.read_csv(\"data/train.csv\")\n",
    "df_test = pd.read_csv(\"data/test.csv\")\n",
    "df_val = pd.read_csv(\"data/val.csv\")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id                                               text  class\n",
       "0   0  @alisachachka не уезжаааааааай. :(❤ я тоже не ...      0\n",
       "1   1  RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...      1\n",
       "2   2  RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...      0\n",
       "3   3  RT @epupybobv: Хочется котлету по-киевски. Зап...      1\n",
       "4   4  @KarineKurganova @Yess__Boss босапопа есбоса н...      1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@alisachachka не уезжаааааааай. :(❤ я тоже не ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @GalyginVadim: Ребята и девчата!\\nВсе в кин...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>RT @ARTEM_KLYUSHIN: Кто ненавидит пробки ретви...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @epupybobv: Хочется котлету по-киевски. Зап...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@KarineKurganova @Yess__Boss босапопа есбоса н...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "exclude = set(punctuation)\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub(\"\\sне\", \"не\", txt)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(preprocess_text)\n",
    "df_val['text'] = df_val['text'].apply(preprocess_text)\n",
    "df_test['text'] = df_test['text'].apply(preprocess_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D, SimpleRNN, LSTM, GRU, Masking\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "text_corpus_train = df_train['text'].values\n",
    "text_corpus_valid = df_val['text'].values\n",
    "text_corpus_test = df_test['text'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer = Tokenizer(num_words=None, \n",
    "                     filters='#$%&()*+-<=>@[\\\\]^_`{|}~\\t\\n',\n",
    "                     lower = False, split = ' ')\n",
    "tokenizer.fit_on_texts(text_corpus_train)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(text_corpus_train)\n",
    "sequences_val = tokenizer.texts_to_sequences(text_corpus_valid)\n",
    "sequences_test = tokenizer.texts_to_sequences(text_corpus_test)\n",
    "\n",
    "word_count = len(tokenizer.index_word) + 1\n",
    "training_length = max([len(i.split()) for i in text_corpus_train])\n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=training_length)\n",
    "X_valid = pad_sequences(sequences_val, maxlen=training_length)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "y_train = df_train['class'].values\n",
    "y_val = df_val['class'].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(input_dim=word_count,\n",
    "              input_length=training_length,\n",
    "              output_dim=30,\n",
    "              trainable=True,\n",
    "              mask_zero=True))\n",
    "model.add(Masking(mask_value=0.0))\n",
    "\n",
    "model.add(SimpleRNN(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "319/319 [==============================] - 42s 132ms/step - loss: 0.5608 - accuracy: 0.6983 - val_loss: 0.4938 - val_accuracy: 0.7511\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 39s 122ms/step - loss: 0.2906 - accuracy: 0.8809 - val_loss: 0.5641 - val_accuracy: 0.7467\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "score = model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "45/45 [==============================] - 0s 10ms/step - loss: 0.5936 - accuracy: 0.7306\n",
      "\n",
      "\n",
      "Test score: 0.5936205983161926\n",
      "Test accuracy: 0.7305911779403687\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Какие проблемы у рекурентных сетей?\n",
    "\n",
    "- затухают градиенты\n",
    "- медленно, нужно всегда дойти до конца\n",
    "\n",
    "Как решить? -> LSTM\n",
    "\n",
    "\n",
    "<img src=\"images/lstm.png\">\n",
    "\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "\n",
    "Давайте, кратко посмотрим как это работает:\n",
    "\n",
    "\n",
    "<img src=\"images/LSTMMaths.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(input_dim=word_count,\n",
    "              input_length=training_length,\n",
    "              output_dim=30,\n",
    "              trainable=True,\n",
    "              mask_zero=True))\n",
    "model.add(Masking(mask_value=0.0))\n",
    "model.add(LSTM(64, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "319/319 [==============================] - 51s 159ms/step - loss: 0.5548 - accuracy: 0.7072 - val_loss: 0.4947 - val_accuracy: 0.7515\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 51s 161ms/step - loss: 0.3274 - accuracy: 0.8638 - val_loss: 0.5365 - val_accuracy: 0.7479\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "score = model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "45/45 [==============================] - 1s 19ms/step - loss: 0.5829 - accuracy: 0.7361\n",
      "\n",
      "\n",
      "Test score: 0.5829179286956787\n",
      "Test accuracy: 0.7361019253730774\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Какие проблемы:\n",
    "\n",
    "- вычислительно сложно -> медленнее\n",
    "- на очень длинных последовательностях все равно затухает градиент\n",
    "\n",
    "\n",
    "Зачем платить больше - уберем некоторые врата (точнее совместим) -> ускоримся, уменьшим число параметров -> GRU\n",
    "\n",
    "\n",
    "<img src=\"images/gru.png\">\n",
    "\n",
    "\n",
    "GRU Math\n",
    "\n",
    "\n",
    "<img src=\"images/GRUMath.png\">\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(input_dim=word_count,\n",
    "              input_length=training_length,\n",
    "              output_dim=30,\n",
    "              trainable=True,\n",
    "              mask_zero=True))\n",
    "model.add(Masking(mask_value=0.0))\n",
    "model.add(GRU(64, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "319/319 [==============================] - 47s 148ms/step - loss: 0.5520 - accuracy: 0.7089 - val_loss: 0.4915 - val_accuracy: 0.7557\n",
      "Epoch 2/10\n",
      "319/319 [==============================] - 46s 146ms/step - loss: 0.3177 - accuracy: 0.8678 - val_loss: 0.5463 - val_accuracy: 0.7474\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "score = model.evaluate(X_valid, y_val, batch_size=512, verbose=1)\n",
    "print('\\n')\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "45/45 [==============================] - 1s 18ms/step - loss: 0.5983 - accuracy: 0.7388\n",
      "\n",
      "\n",
      "Test score: 0.5983427166938782\n",
      "Test accuracy: 0.7388352751731873\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3 подхода:\n",
    "\n",
    "<img src=\"images/RNNCompar.png\">\n",
    "\n",
    "\n",
    "Как регуляризовать?\n",
    "- дропаут\n",
    "- рекурентный дропаут\n",
    "\n",
    "\n",
    "<img src=\"images/Dropouts.png\">"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}