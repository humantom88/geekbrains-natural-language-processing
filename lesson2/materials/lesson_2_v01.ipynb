{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "lesson_2_v01.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVPO5pRfiOkM"
      },
      "source": [
        "### 1. Создание признакового пространства "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4lJbZCliOkT"
      },
      "source": [
        "Алгоритмы машинного обучения не могут напрямую работать с сырым текстом, поэтому необходимо конвертировать текст в наборы цифр (векторы). Это называется извлечением признаков.\n",
        "\n",
        "##### Мешок слов\n",
        "– это популярная и простая техника извлечения признаков, используемая при работе с текстом. Она описывает вхождения каждого слова в текст.\n",
        "\n",
        "Чтобы использовать модель, нам нужно:\n",
        "\n",
        "- Определить словарь известных слов (токенов).\n",
        "- Выбрать степень присутствия известных слов.\n",
        "\n",
        "Любая информация о порядке или структуре слов игнорируется. Вот почему это называется МЕШКОМ слов. Эта модель пытается понять, встречается ли знакомое слово в документе, но не знает, где именно оно встречается.\n",
        "\n",
        "Интуиция подсказывает, что схожие документы имеют схожее содержимое. Также, благодаря содержимому, мы можем узнать кое-что о смысле документа.\n",
        "\n",
        "Пример:\n",
        "Рассмотрим шаги создания этой модели. Мы используем только 4 предложения, чтобы понять, как работает модель. В реальной жизни вы столкнетесь с бОльшими объемами данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0xrkwX7iOkU"
      },
      "source": [
        "documents = [\"I like this movie, it's it's it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eNlvrNkiOkU"
      },
      "source": [
        "Определяем словарь и создаем векторы документа. Соберем все уникальные слова из 4 загруженных предложений, игнорируя регистр, пунктуацию и односимвольные токены. Это и будет наш словарь (известные слова).\n",
        "\n",
        "Для создания словаря можно использовать класс CountVectorizer из библиотеки sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejzjGAaHiOkV"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYcJmtQ4iOkV",
        "outputId": "269ee066-8ac5-4f93-e984-0f814c443545"
      },
      "source": [
        "documents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I like this movie, it's it's it's funny.\",\n",
              " 'I hate this movie.',\n",
              " 'This was awesome! I like it.',\n",
              " 'Nice one. I love it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKlx84IriOkX",
        "outputId": "bbb24922-c5e0-44b0-f9a9-5ffa61af23ad"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "count_vectorizer = CountVectorizer(ngram_range=(2, 2), analyzer='word', binary=True,)\n",
        "\n",
        "# Создаем the Bag-of-Words модель\n",
        "bag_of_words = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Отобразим Bag-of-Words модель как DataFrame\n",
        "feature_names = count_vectorizer.get_feature_names()\n",
        "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>awesome like</th>\n",
              "      <th>hate this</th>\n",
              "      <th>it funny</th>\n",
              "      <th>it it</th>\n",
              "      <th>like it</th>\n",
              "      <th>like this</th>\n",
              "      <th>love it</th>\n",
              "      <th>movie it</th>\n",
              "      <th>nice one</th>\n",
              "      <th>one love</th>\n",
              "      <th>this movie</th>\n",
              "      <th>this was</th>\n",
              "      <th>was awesome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   awesome like  hate this  it funny  it it  like it  like this  love it  \\\n",
              "0             0          0         1      1        0          1        0   \n",
              "1             0          1         0      0        0          0        0   \n",
              "2             1          0         0      0        1          0        0   \n",
              "3             0          0         0      0        0          0        1   \n",
              "\n",
              "   movie it  nice one  one love  this movie  this was  was awesome  \n",
              "0         1         0         0           1         0            0  \n",
              "1         0         0         0           1         0            0  \n",
              "2         0         0         0           0         1            1  \n",
              "3         0         1         1           0         0            0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUUmv9iniOkY"
      },
      "source": [
        "Когда размер словаря увеличивается, вектор документа тоже растет. В примере выше, длина вектора равна количеству известных слов.\n",
        "\n",
        "В некоторых случаях, у нас может быть неимоверно большой объем данных и тогда вектор может состоять из тысяч или миллионов элементов. Более того, каждый документ может содержать лишь малую часть слов из словаря.\n",
        "\n",
        "Как следствие, в векторном представлении будет много нулей. Векторы с большим количеством нулей называются разреженным векторами (sparse vectors), они требуют больше памяти и вычислительных ресурсов. Частично эту проблему можно реить хорошей предобработкой.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iug7wJ-_iOkY"
      },
      "source": [
        "##### Мешок N-грамм\n",
        "\n",
        "Другой, более сложный способ создания словаря – использовать сгруппированные слова. Это изменит размер словаря и даст мешку слов больше деталей о документе. Такой подход называется «N-грамма».\n",
        "\n",
        "N-грамма это последовательность каких-либо сущностей (слов, букв, чисел, цифр и т.д.). В контексте языковых корпусов, под N-граммой обычно понимают последовательность слов. Юниграмма это одно слово, биграмма это последовательность двух слов, триграмма – три слова и так далее. Цифра N обозначает, сколько сгруппированных слов входит в N-грамму. В модель попадают не все возможные N-граммы, а только те, что фигурируют в корпусе.\n",
        "\n",
        "Пример:\n",
        "Рассмотрим такое предложение:The office building is open today\n",
        "\n",
        "Вот его биграммы:\n",
        "\n",
        "- the office\n",
        "- office building\n",
        "- building is\n",
        "- is open\n",
        "- open today\n",
        "\n",
        "Как видно, мешок биграмм – это более действенный подход, чем мешок слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euFRvj3PiOkZ"
      },
      "source": [
        "BPE\n",
        "\"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
        "\n",
        "nn - t1\n",
        "uy - t2\n",
        "t1u - t3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnlYIf7JiOkZ",
        "outputId": "8f60d136-ddbf-4240-b22f-88f200f20724"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
        "tokenized = text.split()\n",
        "bigrams = ngrams(tokenized, 3)\n",
        "list(bigrams)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'like', 'this'),\n",
              " ('like', 'this', 'movie,'),\n",
              " ('this', 'movie,', \"it's\"),\n",
              " ('movie,', \"it's\", 'funny.'),\n",
              " (\"it's\", 'funny.', 'I'),\n",
              " ('funny.', 'I', 'hate'),\n",
              " ('I', 'hate', 'this'),\n",
              " ('hate', 'this', 'movie.'),\n",
              " ('this', 'movie.', 'This'),\n",
              " ('movie.', 'This', 'was'),\n",
              " ('This', 'was', 'awesome!'),\n",
              " ('was', 'awesome!', 'I'),\n",
              " ('awesome!', 'I', 'like'),\n",
              " ('I', 'like', 'it.'),\n",
              " ('like', 'it.', 'Nice'),\n",
              " ('it.', 'Nice', 'one.'),\n",
              " ('Nice', 'one.', 'I'),\n",
              " ('one.', 'I', 'love'),\n",
              " ('I', 'love', 'it.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag-k52N-iOka"
      },
      "source": [
        "##### TF-IDF\n",
        "\n",
        "У частотного скоринга есть проблема: слова с наибольшей частотностью имеют, соответственно, наибольшую оценку. В этих словах может быть не так много информационного выигрыша для модели, как в менее частых словах. Один из способов исправить ситуацию – понижать оценку слова, которое часто встречается во всех схожих документах. Это называется TF-IDF.\n",
        "\n",
        "TF-IDF (сокращение от term frequency — inverse document frequency) – это статистическая мера для оценки важности слова в документе, который является частью коллекции или корпуса.\n",
        "\n",
        "Скоринг по TF-IDF растет пропорционально частоте появления слова в документе, но это компенсируется количеством документов, содержащих это слово."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SFyq9wziOka"
      },
      "source": [
        "<img src='image/tf_idf.PNG'>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zpDiNxmiOka",
        "outputId": "dc2e85f9-42ae-46f6-f07e-f2f403b278e4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "document = [\"I like this movie, it's funny funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "values = tfidf_vectorizer.fit_transform(document)\n",
        "\n",
        "# Show the Model as a pandas DataFrame\n",
        "feature_names = tfidf_vectorizer.get_feature_names()\n",
        "pd.DataFrame(values.toarray(), columns = feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>awesome</th>\n",
              "      <th>funny</th>\n",
              "      <th>hate</th>\n",
              "      <th>it</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>movie</th>\n",
              "      <th>nice</th>\n",
              "      <th>one</th>\n",
              "      <th>this</th>\n",
              "      <th>was</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.812578</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.259329</td>\n",
              "      <td>0.320323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.320323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.259329</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.702035</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.553492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448100</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.539445</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344321</td>\n",
              "      <td>0.425305</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344321</td>\n",
              "      <td>0.539445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.345783</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.541736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.541736</td>\n",
              "      <td>0.541736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    awesome     funny      hate        it      like      love     movie  \\\n",
              "0  0.000000  0.812578  0.000000  0.259329  0.320323  0.000000  0.320323   \n",
              "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
              "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
              "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
              "\n",
              "       nice       one      this       was  \n",
              "0  0.000000  0.000000  0.259329  0.000000  \n",
              "1  0.000000  0.000000  0.448100  0.000000  \n",
              "2  0.000000  0.000000  0.344321  0.539445  \n",
              "3  0.541736  0.541736  0.000000  0.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fad_6UPeiOkb",
        "outputId": "e0acf3a4-01c9-4212-b2e7-4c3836c0df5f"
      },
      "source": [
        "tfidf_vectorizer.idf_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.91629073, 1.91629073, 1.91629073, 1.22314355, 1.51082562,\n",
              "       1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.22314355,\n",
              "       1.91629073])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I3IMD7qiOkc"
      },
      "source": [
        "##### HashingVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-fZKZqHiOkd"
      },
      "source": [
        "Подсчеты и частоты могут быть очень полезны, но одним из ограничений этих методов является то, что словарный запас может стать очень большим. Это, в свою очередь, потребует больших векторов для кодирования документов и налагает большие требования к памяти и замедляет алгоритмы.\n",
        "\n",
        "Умный обходной путь - использовать односторонний хэш слов, чтобы преобразовать их в целые числа. Умная часть заключается в том, что словарь не требуется, и вы можете выбрать произвольный длинный вектор фиксированной длины. Недостатком является то, что хеш является односторонней функцией, поэтому нет способа преобразовать кодировку обратно в слово (что может не иметь значения для многих контролируемых задач обучения).HashingVectorizer класс реализует этот подход, который можно использовать для последовательного хеширования слов, а затем для токенизации и кодирования документов по мере необходимости.\n",
        "\n",
        "Пример ниже демонстрирует HashingVectorizer для кодирования одного документа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFPub3fyiOkd",
        "outputId": "4ab88d92-5224-42c8-ce26-0f6649c37513"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "document = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
        "vectorizer = HashingVectorizer(n_features=2**4,)\n",
        "values = vectorizer.fit_transform(document)\n",
        "print(values.shape)\n",
        "print(values.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 16)\n",
            "[[ 0.37796447  0.          0.          0.          0.          0.\n",
            "   0.         -0.37796447  0.          0.         -0.37796447  0.\n",
            "   0.          0.          0.75592895  0.        ]\n",
            " [ 0.          0.          0.         -0.57735027  0.          0.\n",
            "   0.          0.          0.          0.         -0.57735027  0.\n",
            "   0.          0.          0.57735027  0.        ]\n",
            " [ 0.          0.4472136   0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.89442719  0.        ]\n",
            " [ 0.          0.          0.          0.         -0.5         0.\n",
            "   0.          0.          0.         -0.5         0.          0.\n",
            "   0.          0.5         0.5         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6iI_aLHiOke"
      },
      "source": [
        "Выполнение примера кодирует образец документа как разреженный массив из 16 элементов. Значения закодированного документа соответствуют нормализованному количеству слов по умолчанию в диапазоне от -1 до 1, но могут быть сделаны простые целочисленные счетчики путем изменения конфигурации по умолчанию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umasa6KWiOkf"
      },
      "source": [
        "##### Для чего нужны Vectorizers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXz9VqlOiOkf"
      },
      "source": [
        "Алгоритмы машинного обучения не могут напрямую работать с сырым текстом, поэтому необходимо конвертировать текст в наборы цифр (векторы). Уже с векторным представлением можно производить классификацию, к примеру."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg00Asa_iOkf",
        "outputId": "91d81952-4a68-4b79-e8f1-2ba64ed6e501"
      },
      "source": [
        "# Загружаем данные\n",
        "data = open('corpus').read()\n",
        "labels, texts = [], []\n",
        "for i, line in enumerate(data.split(\"\\n\")):\n",
        "    content = line.split()\n",
        "    labels.append(content[0])\n",
        "    texts.append(\" \".join(content[1:]))\n",
        "\n",
        "# создаем df\n",
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = texts\n",
        "trainDF['label'] = labels\n",
        "trainDF.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "      <td>__label__2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       label\n",
              "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
              "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
              "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
              "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
              "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnW0dk_4iOkg",
        "outputId": "415f2c8d-f6c0-419b-d40e-2dc7d0522dbb"
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model,\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# labelEncode целевую переменную\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)\n",
        "\n",
        "\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)\n",
        "\n",
        "    classifier = linear_model.LogisticRegression()\n",
        "    classifier.fit(xtrain_count, train_y)\n",
        "    predictions = classifier.predict(xvalid_count)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZSwXcoTiOkg"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nlVWS_MiOkh",
        "outputId": "970c9930-883b-4ee8-c2bc-194ec0547382"
      },
      "source": [
        "accuracy_score(valid_y, predictionsictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8624"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVQjKh_HiOkh",
        "outputId": "1976bd8c-ec7e-45ce-b65e-8607a9509127"
      },
      "source": [
        "trainDF['label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__label__1    5097\n",
              "__label__2    4903\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtWmKgPiiOki"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}